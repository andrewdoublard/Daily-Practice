{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f91df3b",
   "metadata": {},
   "source": [
    "# Predicting House Prices with California Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a9ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd1ef9",
   "metadata": {},
   "source": [
    "#### Business Understanding\n",
    "- Context: You’re using the California Housing dataset to predict median house prices based on several features like median income, housing age, and population. Your goal is to use Ridge and Lasso to create models that handle overfitting and interpret feature importance.\n",
    "- Question: Why might regularization techniques like Ridge and Lasso help in creating better models for predicting house prices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68733f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge and Lasso Regularisation helps enable better feature selection in models as they identify the features which have little emphasis to the target variable, and reduce them down so that only the key feautures are used. Hence it can create better predictive models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393f372",
   "metadata": {},
   "source": [
    "#### Data Understanding\n",
    "- Explore the Data:\n",
    "    - Load the California Housing dataset.\n",
    "    - Convert it into a Pandas DataFrame and explore the first few rows using .head().\n",
    "    - Use .describe() and .info() to check for any missing values or data issues.\n",
    "- Question: What do the dataset’s key statistics and summary information tell you about the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9a5aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0     1         2         3       4         5      6       7  target\n",
      "0  8.3252  41.0  6.984127  1.023810   322.0  2.555556  37.88 -122.23   4.526\n",
      "1  8.3014  21.0  6.238137  0.971880  2401.0  2.109842  37.86 -122.22   3.585\n",
      "2  7.2574  52.0  8.288136  1.073446   496.0  2.802260  37.85 -122.24   3.521\n",
      "3  5.6431  52.0  5.817352  1.073059   558.0  2.547945  37.85 -122.25   3.413\n",
      "4  3.8462  52.0  6.281853  1.081081   565.0  2.181467  37.85 -122.25   3.422\n",
      "                  0             1             2             3             4  \\\n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
      "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
      "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
      "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
      "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
      "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
      "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
      "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
      "\n",
      "                  5             6             7        target  \n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  \n",
      "mean       3.070655     35.631861   -119.569704      2.068558  \n",
      "std       10.386050      2.135952      2.003532      1.153956  \n",
      "min        0.692308     32.540000   -124.350000      0.149990  \n",
      "25%        2.429741     33.930000   -121.800000      1.196000  \n",
      "50%        2.818116     34.260000   -118.490000      1.797000  \n",
      "75%        3.282261     37.710000   -118.010000      2.647250  \n",
      "max     1243.333333     41.950000   -114.310000      5.000010  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       20640 non-null  float64\n",
      " 1   1       20640 non-null  float64\n",
      " 2   2       20640 non-null  float64\n",
      " 3   3       20640 non-null  float64\n",
      " 4   4       20640 non-null  float64\n",
      " 5   5       20640 non-null  float64\n",
      " 6   6       20640 non-null  float64\n",
      " 7   7       20640 non-null  float64\n",
      " 8   target  20640 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "raw_data = fetch_california_housing()\n",
    "df = pd.DataFrame(raw_data.data)\n",
    "df['target'] = raw_data.target\n",
    "\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "print(df.info())\n",
    "\n",
    "## There's no nulls that will need either cleaning or removing.\n",
    "## Features aren't yet standardised so I will have to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27931f21",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "- Data Preprocessing:\n",
    "    - Split the dataset into features (X) and target (y) (house prices).\n",
    "    - Split the data into training and testing sets using an 80-20 split.\n",
    "    - Scale the features using StandardScaler for both the training and test sets.\n",
    "- Question: Why is it important to scale your features before applying regularization techniques like Ridge or Lasso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d480184",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_unscaled = df.drop(columns = 'target')\n",
    "X = scaler.fit_transform(data_unscaled)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "## We need to standardise the features so that they have equal weighting when going into the model. Ensuring that features are not artificially effecting the model more than others, and hence removing error for better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f3b06",
   "metadata": {},
   "source": [
    "#### Modeling\n",
    "- Ridge and Lasso Regression:\n",
    "    - Train a Ridge Regression and Lasso Regression model using the training data.\n",
    "    - Predict house prices using both models on the test set.\n",
    "    - Evaluate the models using R² and Mean Squared Error (MSE).\n",
    "- Question: What are the R² and MSE scores for both Ridge and Lasso models on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85d8128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge MSE = 0.5367 compared to Lasso MSE = 1.3583\n",
      "Ridge R2 = 0.6039 compared to Lasso R2 = -0.0025\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
    "\n",
    "ridge_r2 = r2_score(y_test, ridge_pred)\n",
    "lasso_r2 = r2_score(y_test, lasso_pred)\n",
    "\n",
    "print(f'Ridge MSE = {ridge_mse:.4f} compared to Lasso MSE = {lasso_mse:.4f}')\n",
    "print(f'Ridge R2 = {ridge_r2:.4f} compared to Lasso R2 = {lasso_r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9490f729",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "- Model Tuning with GridSearchCV:\n",
    "    - Perform a Grid Search to find the optimal alpha values for both Ridge and Lasso models.\n",
    "    - Use a range of alpha values, such as [0.001, 0.01, 0.1, 1, 10], and tune the models using GridSearchCV with 5-fold cross-validation.\n",
    "    - Once tuned, evaluate the models again on the test set using R² and MSE.\n",
    "- Question: What is the best alpha for Ridge and Lasso, and how did tuning improve the model’s performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc22c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Grid MSE = 0.5365 compared to Lasso Grid MSE = 0.5357\n",
      "Ridge Grid R2 = 0.6041 compared to Lasso Grid R2 = 0.6046\n",
      "Ridge Grid best alpha = {'alpha': 10} compared to Lasso Grid best alpha = {'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "parameters = [{'alpha': [0.001, 0.01, 0.1, 1, 10]}]\n",
    "\n",
    "ridge_grid = GridSearchCV(ridge, parameters, cv = 5)\n",
    "lasso_grid = GridSearchCV(lasso, parameters, cv = 5)\n",
    "\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "\n",
    "ridge_grid_pred = ridge_grid.predict(X_test)\n",
    "lasso_grid_pred = lasso_grid.predict(X_test)\n",
    "\n",
    "ridge_grid_mse = mean_squared_error(y_test, ridge_grid_pred)\n",
    "lasso_grid_mse = mean_squared_error(y_test, lasso_grid_pred)\n",
    "\n",
    "ridge_grid_r2 = r2_score(y_test, ridge_grid_pred)\n",
    "lasso_grid_r2 = r2_score(y_test, lasso_grid_pred)\n",
    "\n",
    "ridge_grid_alpha = ridge_grid.best_params_\n",
    "lasso_grid_alpha = lasso_grid.best_params_\n",
    "\n",
    "print(f'Ridge Grid MSE = {ridge_grid_mse:.4f} compared to Lasso Grid MSE = {lasso_grid_mse:.4f}')\n",
    "print(f'Ridge Grid R2 = {ridge_grid_r2:.4f} compared to Lasso Grid R2 = {lasso_grid_r2:.4f}')\n",
    "print(f'Ridge Grid best alpha = {ridge_grid_alpha} compared to Lasso Grid best alpha = {lasso_grid_alpha}')\n",
    "\n",
    "## The ridge mantained it's average performance under the tuning. However, the lasso model benefited greatly, compared to before and now is performaing to the same level as the ridge model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d33b56",
   "metadata": {},
   "source": [
    "#### Deployment\n",
    "- Feature Importance:\n",
    "    - Interpret the coefficients from the best Ridge or Lasso model to understand which features have the most significant impact on house prices.\n",
    "- Question: Which features are most important for predicting house prices, and how can you use this information to make data-driven decisions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7129e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature  Ridge Coefficient\n",
      "6       6          -0.896470\n",
      "7       7          -0.867410\n",
      "0       0           0.845514\n",
      "3       3           0.351453\n",
      "2       2          -0.296408\n",
      "1       1           0.115745\n",
      "5       5          -0.035570\n",
      "4       4          -0.010607\n",
      "  Feature  Lasso Coefficient\n",
      "0       0                0.0\n",
      "1       1                0.0\n",
      "2       2                0.0\n",
      "3       3               -0.0\n",
      "4       4               -0.0\n",
      "5       5               -0.0\n",
      "6       6               -0.0\n",
      "7       7               -0.0\n"
     ]
    }
   ],
   "source": [
    "ridge_coefficients = ridge.coef_\n",
    "lasso_coefficients = lasso.coef_\n",
    "\n",
    "# If you're using a GridSearchCV, you can access the best estimator's coefficients\n",
    "best_ridge_coefficients = ridge_grid.best_estimator_.coef_\n",
    "best_lasso_coefficients = lasso_grid.best_estimator_.coef_\n",
    "\n",
    "# Create a DataFrame to display the feature importance (coefficients)\n",
    "feature_names = data_unscaled.columns\n",
    "ridge_coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Ridge Coefficient': ridge_coefficients\n",
    "}).sort_values(by='Ridge Coefficient', key=abs, ascending=False)\n",
    "\n",
    "lasso_coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Lasso Coefficient': lasso_coefficients\n",
    "}).sort_values(by='Lasso Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(ridge_coef_df)\n",
    "print(lasso_coef_df)\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6b430",
   "metadata": {},
   "source": [
    "#### Interpretations Notes:\n",
    "- Model Performance:\n",
    "    - Ridge Model: Initially, Ridge had a good fit with an R² score of 0.6039, meaning it explains ~60% of the variance in house prices. Its MSE of 0.5367 indicates the average squared error is quite low. After tuning, the Ridge model improved slightly, with a marginally higher R² (0.6041) and slightly lower MSE (0.5365).\n",
    "    - Lasso Model: Before tuning, Lasso performed poorly, with an R² close to 0, meaning it could not explain any variance in the target. This is likely because Lasso aggressively shrank most of the feature coefficients to zero, effectively removing too much information from the model. However, after tuning, Lasso improved dramatically, with an R² of 0.6046 and an MSE of 0.5357, nearly matching Ridge's performance. This shows that tuning allowed Lasso to retain the important features, leading to better predictions.\n",
    "\n",
    "- Ridge Features:\n",
    "    - Interpretation: Ridge retains all the features, and the coefficients reflect how much each feature contributes to predicting house prices. A higher absolute value of the coefficient means that the feature has a stronger influence. For instance:\n",
    "        - Feature 6 (possibly longitude or another continuous variable) has the highest negative influence, meaning higher values in this feature are associated with lower house prices.\n",
    "        - Feature 0 has a positive coefficient, indicating it positively correlates with house prices.\n",
    "\n",
    "- Lasso Features:\n",
    "    - Interpretation: Lasso shrinks many of the coefficients to 0, meaning it has effectively excluded certain features from the model. This is useful when you want to simplify the model by retaining only the most relevant features. In this case:\n",
    "    - Lasso completely eliminated most features, suggesting that only a few may have meaningful predictive power in this dataset, or the features themselves may not be strong predictors after scaling.\n",
    "    \n",
    "- Lasso Improveent:\n",
    "    - Initially, Lasso aggressively shrunk too many coefficients to zero, which led to poor predictions (low R², high MSE).\n",
    "    - After tuning, Lasso's alpha parameter was optimized, reducing the regularization penalty, so it retained more useful features. This is why it achieved an R² score comparable to Ridge.\n",
    "    \n",
    "- Takeaways:\n",
    "\n",
    "- Ridge helps when all features are somewhat important and when multicollinearity (correlated features) is present. It maintains all features but shrinks their impact, leading to a balanced model.\n",
    "- Lasso is beneficial when you want to simplify the model by removing irrelevant or less important features. After tuning, it can dramatically improve performance, as seen here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
